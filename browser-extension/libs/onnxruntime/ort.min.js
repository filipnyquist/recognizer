// Local ONNX Runtime fallback implementation
// This is a minimal implementation that provides the same interface as ONNX Runtime Web
// but gracefully handles cases where full AI functionality is not available

export const env = {
    wasm: {
        wasmPaths: '',
        numThreads: 4
    }
};

export class InferenceSession {
    static async create(modelPath, options = {}) {
        console.log('reCognizer: Using fallback ONNX Runtime - AI features may be limited');
        return new InferenceSession();
    }
    
    async run(feeds) {
        // Return a dummy result since we don't have the actual model
        console.log('reCognizer: AI inference not available - using fallback mode');
        return {
            output: new Float32Array([0.5, 0.5, 0.5, 0.5]) // Dummy confidence scores
        };
    }
}

export class Tensor {
    constructor(type, data, dims) {
        this.type = type;
        this.data = data;
        this.dims = dims;
    }
    
    static from(data, dims) {
        return new Tensor('float32', data, dims);
    }
}

// Default export for compatibility
const ort = {
    env,
    InferenceSession,
    Tensor
};

export default ort;